# src/evaluator.py
# Module ƒë√°nh gi√° to√†n di·ªán cho b√†i to√°n d·ª± ƒëo√°n Vietlott

import numpy as np
from typing import Dict, List, Tuple, Optional
from collections import defaultdict
from sklearn.metrics import precision_score, recall_score, f1_score

class LotteryEvaluator:
    """
    Class ƒë√°nh gi√° hi·ªáu su·∫•t model v·ªõi nhi·ªÅu metrics kh√°c nhau
    ph√π h·ª£p cho b√†i to√°n d·ª± ƒëo√°n x·ªï s·ªë
    """
    
    def __init__(self, top_k: int = 8):
        """
        Args:
            top_k: S·ªë l∆∞·ª£ng s·ªë ƒë∆∞·ª£c ƒë·ªÅ xu·∫•t (m·∫∑c ƒë·ªãnh 8)
        """
        self.top_k = top_k
        self.results = {}
    
    def evaluate_hits(self, predictions: np.ndarray, actuals: np.ndarray) -> Dict:
        """
        ƒê√°nh gi√° s·ªë l∆∞·ª£ng s·ªë tr√∫ng (Hits)
        
        Args:
            predictions: Array (n_samples, 45) - x√°c su·∫•t cho 45 s·ªë
            actuals: Array (n_samples, 45) - one-hot encoding c·ªßa s·ªë th·ª±c t·∫ø
            
        Returns:
            Dict ch·ª©a c√°c metrics v·ªÅ hits
        """
        n_samples = len(predictions)
        hits_per_sample = []
        hit_rates = []
        
        for i in range(n_samples):
            # L·∫•y top-K s·ªë c√≥ x√°c su·∫•t cao nh·∫•t
            top_indices = np.argsort(predictions[i])[-self.top_k:][::-1]
            pred_nums = set(top_indices)
            
            # S·ªë th·ª±c t·∫ø
            actual_indices = np.where(actuals[i] == 1)[0]
            actual_nums = set(actual_indices)
            
            # S·ªë tr√∫ng
            hits = len(pred_nums & actual_nums)
            hits_per_sample.append(hits)
            hit_rates.append(hits / self.top_k)
        
        hits_array = np.array(hits_per_sample)
        
        return {
            'mean_hits': np.mean(hits_array),
            'std_hits': np.std(hits_array),
            'median_hits': np.median(hits_array),
            'max_hits': np.max(hits_array),
            'min_hits': np.min(hits_array),
            'mean_hit_rate': np.mean(hit_rates),
            'hits_distribution': self._get_distribution(hits_array),
            'hits_per_sample': hits_per_sample
        }
    
    def evaluate_precision_recall(self, predictions: np.ndarray, actuals: np.ndarray) -> Dict:
        """
        ƒê√°nh gi√° Precision, Recall, F1-score
        
        Args:
            predictions: Array (n_samples, 45) - x√°c su·∫•t
            actuals: Array (n_samples, 45) - one-hot encoding
            
        Returns:
            Dict ch·ª©a precision, recall, f1
        """
        n_samples = len(predictions)
        precisions = []
        recalls = []
        f1_scores = []
        
        for i in range(n_samples):
            # Top-K predictions (binary)
            top_indices = np.argsort(predictions[i])[-self.top_k:][::-1]
            pred_binary = np.zeros(45)
            pred_binary[top_indices] = 1
            
            # Actual (binary)
            actual_binary = actuals[i]
            
            # T√≠nh metrics
            precision = precision_score(actual_binary, pred_binary, zero_division=0)
            recall = recall_score(actual_binary, pred_binary, zero_division=0)
            f1 = f1_score(actual_binary, pred_binary, zero_division=0)
            
            precisions.append(precision)
            recalls.append(recall)
            f1_scores.append(f1)
        
        return {
            'mean_precision': np.mean(precisions),
            'mean_recall': np.mean(recalls),
            'mean_f1': np.mean(f1_scores),
            'std_precision': np.std(precisions),
            'std_recall': np.std(recalls),
            'std_f1': np.std(f1_scores)
        }
    
    def evaluate_rank_metrics(self, predictions: np.ndarray, actuals: np.ndarray) -> Dict:
        """
        ƒê√°nh gi√° d·ª±a tr√™n ranking (v·ªã tr√≠ c·ªßa s·ªë th·ª±c t·∫ø trong ranking)
        
        Args:
            predictions: Array (n_samples, 45) - x√°c su·∫•t
            actuals: Array (n_samples, 45) - one-hot encoding
            
        Returns:
            Dict ch·ª©a c√°c metrics v·ªÅ ranking
        """
        n_samples = len(predictions)
        ranks = []
        top_k_ranks = []
        
        for i in range(n_samples):
            # S·∫Øp x·∫øp theo x√°c su·∫•t gi·∫£m d·∫ßn
            sorted_indices = np.argsort(predictions[i])[::-1]
            
            # S·ªë th·ª±c t·∫ø
            actual_indices = np.where(actuals[i] == 1)[0]
            
            # V·ªã tr√≠ c·ªßa m·ªói s·ªë th·ª±c t·∫ø trong ranking
            sample_ranks = []
            for actual_idx in actual_indices:
                rank = np.where(sorted_indices == actual_idx)[0][0] + 1  # 1-indexed
                sample_ranks.append(rank)
                ranks.append(rank)
                
                # Ch·ªâ t√≠nh n·∫øu trong top-K
                if rank <= self.top_k:
                    top_k_ranks.append(rank)
        
        if len(ranks) == 0:
            return {
                'mean_rank': 0,
                'median_rank': 0,
                'mean_rank_in_topk': 0,
                'coverage_at_k': 0
            }
        
        ranks_array = np.array(ranks)
        coverage = len(top_k_ranks) / len(ranks) if len(ranks) > 0 else 0
        
        return {
            'mean_rank': np.mean(ranks_array),
            'median_rank': np.median(ranks_array),
            'std_rank': np.std(ranks_array),
            'mean_rank_in_topk': np.mean(top_k_ranks) if len(top_k_ranks) > 0 else 0,
            'coverage_at_k': coverage,  # T·ª∑ l·ªá s·ªë th·ª±c t·∫ø n·∫±m trong top-K
            'min_rank': np.min(ranks_array),
            'max_rank': np.max(ranks_array)
        }
    
    def evaluate_probability_calibration(self, predictions: np.ndarray, actuals: np.ndarray) -> Dict:
        """
        ƒê√°nh gi√° ƒë·ªô ch√≠nh x√°c c·ªßa x√°c su·∫•t (calibration)
        
        Args:
            predictions: Array (n_samples, 45) - x√°c su·∫•t
            actuals: Array (n_samples, 45) - one-hot encoding
            
        Returns:
            Dict ch·ª©a c√°c metrics v·ªÅ calibration
        """
        n_samples = len(predictions)
        
        # Chia x√°c su·∫•t th√†nh c√°c bins
        bins = np.linspace(0, 1, 11)  # 10 bins: 0-0.1, 0.1-0.2, ...
        bin_counts = np.zeros(len(bins) - 1)
        bin_actuals = np.zeros(len(bins) - 1)
        
        for i in range(n_samples):
            for j in range(45):
                prob = predictions[i, j]
                actual = actuals[i, j]
                
                # T√¨m bin
                bin_idx = np.digitize(prob, bins) - 1
                bin_idx = max(0, min(bin_idx, len(bins) - 2))
                
                bin_counts[bin_idx] += 1
                bin_actuals[bin_idx] += actual
        
        # T√≠nh empirical probability cho m·ªói bin
        empirical_probs = []
        predicted_probs = []
        for i in range(len(bins) - 1):
            if bin_counts[i] > 0:
                empirical = bin_actuals[i] / bin_counts[i]
                predicted = (bins[i] + bins[i+1]) / 2
                empirical_probs.append(empirical)
                predicted_probs.append(predicted)
        
        # T√≠nh calibration error (Brier score)
        brier_scores = []
        for i in range(n_samples):
            for j in range(45):
                prob = predictions[i, j]
                actual = actuals[i, j]
                brier = (prob - actual) ** 2
                brier_scores.append(brier)
        
        return {
            'mean_brier_score': np.mean(brier_scores),
            'calibration_error': np.mean(np.abs(np.array(empirical_probs) - np.array(predicted_probs))) if len(empirical_probs) > 0 else 0,
            'empirical_probs': empirical_probs,
            'predicted_probs': predicted_probs
        }
    
    def evaluate_coverage(self, predictions: np.ndarray, actuals: np.ndarray) -> Dict:
        """
        ƒê√°nh gi√° coverage - bao nhi√™u s·ªë th·ª±c t·∫ø ƒë∆∞·ª£c d·ª± ƒëo√°n
        
        Args:
            predictions: Array (n_samples, 45) - x√°c su·∫•t
            actuals: Array (n_samples, 45) - one-hot encoding
            
        Returns:
            Dict ch·ª©a coverage metrics
        """
        n_samples = len(predictions)
        unique_actuals = set()
        unique_predicted = set()
        coverage_per_sample = []
        
        for i in range(n_samples):
            # S·ªë th·ª±c t·∫ø
            actual_indices = np.where(actuals[i] == 1)[0]
            unique_actuals.update(actual_indices)
            
            # Top-K predicted
            top_indices = np.argsort(predictions[i])[-self.top_k:][::-1]
            unique_predicted.update(top_indices)
            
            # Coverage cho sample n√†y
            coverage = len(set(actual_indices) & set(top_indices)) / len(actual_indices) if len(actual_indices) > 0 else 0
            coverage_per_sample.append(coverage)
        
        total_coverage = len(unique_actuals & unique_predicted) / len(unique_actuals) if len(unique_actuals) > 0 else 0
        
        return {
            'mean_coverage': np.mean(coverage_per_sample),
            'total_coverage': total_coverage,
            'unique_actuals_count': len(unique_actuals),
            'unique_predicted_count': len(unique_predicted),
            'overlap_count': len(unique_actuals & unique_predicted)
        }
    
    def compare_with_baseline(self, predictions: np.ndarray, actuals: np.ndarray) -> Dict:
        """
        So s√°nh v·ªõi baseline (random v√† frequency-based)
        
        Args:
            predictions: Array (n_samples, 45) - x√°c su·∫•t t·ª´ model
            actuals: Array (n_samples, 45) - one-hot encoding
            
        Returns:
            Dict so s√°nh v·ªõi baselines
        """
        n_samples = len(predictions)
        
        # Baseline 1: Random
        random_hits = []
        for i in range(n_samples):
            random_pred = np.random.choice(45, size=self.top_k, replace=False)
            actual_indices = np.where(actuals[i] == 1)[0]
            hits = len(set(random_pred) & set(actual_indices))
            random_hits.append(hits)
        
        # Baseline 2: Frequency-based (d·ª±a tr√™n t·∫ßn su·∫•t trong training)
        # T√≠nh t·∫ßn su·∫•t t·ª´ actuals
        freq = np.sum(actuals, axis=0)
        freq_normalized = freq / (np.sum(freq) + 1e-10)
        
        freq_hits = []
        for i in range(n_samples):
            # Ch·ªçn top-K s·ªë c√≥ t·∫ßn su·∫•t cao nh·∫•t
            top_freq_indices = np.argsort(freq_normalized)[-self.top_k:][::-1]
            actual_indices = np.where(actuals[i] == 1)[0]
            hits = len(set(top_freq_indices) & set(actual_indices))
            freq_hits.append(hits)
        
        # Model hits
        model_hits = []
        for i in range(n_samples):
            top_indices = np.argsort(predictions[i])[-self.top_k:][::-1]
            actual_indices = np.where(actuals[i] == 1)[0]
            hits = len(set(top_indices) & set(actual_indices))
            model_hits.append(hits)
        
        return {
            'model_mean_hits': np.mean(model_hits),
            'random_mean_hits': np.mean(random_hits),
            'frequency_mean_hits': np.mean(freq_hits),
            'improvement_over_random': np.mean(model_hits) - np.mean(random_hits),
            'improvement_over_frequency': np.mean(model_hits) - np.mean(freq_hits),
            'improvement_over_random_pct': ((np.mean(model_hits) - np.mean(random_hits)) / np.mean(random_hits) * 100) if np.mean(random_hits) > 0 else 0
        }
    
    def comprehensive_evaluate(self, predictions: np.ndarray, actuals: np.ndarray) -> Dict:
        """
        ƒê√°nh gi√° to√†n di·ªán v·ªõi t·∫•t c·∫£ metrics
        
        Args:
            predictions: Array (n_samples, 45) - x√°c su·∫•t
            actuals: Array (n_samples, 45) - one-hot encoding
            
        Returns:
            Dict ch·ª©a t·∫•t c·∫£ metrics
        """
        results = {}
        
        print("üìä ƒêang ƒë√°nh gi√° v·ªõi nhi·ªÅu metrics...")
        
        # 1. Hits metrics
        print("   ‚Æû ƒêang t√≠nh Hits metrics...")
        results['hits'] = self.evaluate_hits(predictions, actuals)
        
        # 2. Precision/Recall/F1
        print("   ‚Æû ƒêang t√≠nh Precision/Recall/F1...")
        results['precision_recall'] = self.evaluate_precision_recall(predictions, actuals)
        
        # 3. Rank metrics
        print("   ‚Æû ƒêang t√≠nh Rank metrics...")
        results['rank'] = self.evaluate_rank_metrics(predictions, actuals)
        
        # 4. Coverage
        print("   ‚Æû ƒêang t√≠nh Coverage...")
        results['coverage'] = self.evaluate_coverage(predictions, actuals)
        
        # 5. Probability calibration
        print("   ‚Æû ƒêang t√≠nh Probability Calibration...")
        results['calibration'] = self.evaluate_probability_calibration(predictions, actuals)
        
        # 6. Baseline comparison
        print("   ‚Æû ƒêang so s√°nh v·ªõi Baseline...")
        results['baseline'] = self.compare_with_baseline(predictions, actuals)
        
        self.results = results
        return results
    
    def print_evaluation_report(self, results: Optional[Dict] = None):
        """
        In b√°o c√°o ƒë√°nh gi√° ƒë·∫πp
        
        Args:
            results: Dict k·∫øt qu·∫£ (n·∫øu None th√¨ d√πng self.results)
        """
        if results is None:
            results = self.results
        
        if not results:
            print("‚ö†Ô∏è Ch∆∞a c√≥ k·∫øt qu·∫£ ƒë√°nh gi√°. H√£y ch·∫°y comprehensive_evaluate() tr∆∞·ªõc.")
            return
        
        print("\n" + "="*80)
        print("üìä B√ÅO C√ÅO ƒê√ÅNH GI√Å TO√ÄN DI·ªÜN")
        print("="*80)
        
        # 1. Hits Metrics
        hits = results.get('hits', {})
        print("\nüéØ 1. HITS METRICS (S·ªë tr√∫ng)")
        print("-" * 80)
        print(f"   Trung b√¨nh s·ªë tr√∫ng/k·ª≥:     {hits.get('mean_hits', 0):.3f} ¬± {hits.get('std_hits', 0):.3f}")
        print(f"   Median s·ªë tr√∫ng:            {hits.get('median_hits', 0):.2f}")
        print(f"   Min/Max:                     {hits.get('min_hits', 0)} / {hits.get('max_hits', 0)}")
        print(f"   Hit Rate (tr√∫ng/K):         {hits.get('mean_hit_rate', 0)*100:.2f}%")
        print(f"   Ph√¢n ph·ªëi:                  {hits.get('hits_distribution', {})}")
        
        # 2. Precision/Recall/F1
        pr = results.get('precision_recall', {})
        print("\nüìà 2. PRECISION/RECALL/F1")
        print("-" * 80)
        print(f"   Precision:                   {pr.get('mean_precision', 0):.4f} ¬± {pr.get('std_precision', 0):.4f}")
        print(f"   Recall:                      {pr.get('mean_recall', 0):.4f} ¬± {pr.get('std_recall', 0):.4f}")
        print(f"   F1-Score:                    {pr.get('mean_f1', 0):.4f} ¬± {pr.get('std_f1', 0):.4f}")
        
        # 3. Rank Metrics
        rank = results.get('rank', {})
        print("\nüìä 3. RANK METRICS")
        print("-" * 80)
        print(f"   Mean Rank:                   {rank.get('mean_rank', 0):.2f}")
        print(f"   Median Rank:                 {rank.get('median_rank', 0):.2f}")
        print(f"   Mean Rank trong Top-{self.top_k}: {rank.get('mean_rank_in_topk', 0):.2f}")
        print(f"   Coverage t·∫°i Top-{self.top_k}:   {rank.get('coverage_at_k', 0)*100:.2f}%")
        
        # 4. Coverage
        coverage = results.get('coverage', {})
        print("\nüéØ 4. COVERAGE")
        print("-" * 80)
        print(f"   Mean Coverage:               {coverage.get('mean_coverage', 0)*100:.2f}%")
        print(f"   Total Coverage:              {coverage.get('total_coverage', 0)*100:.2f}%")
        print(f"   S·ªë th·ª±c t·∫ø unique:           {coverage.get('unique_actuals_count', 0)}")
        print(f"   S·ªë d·ª± ƒëo√°n unique:           {coverage.get('unique_predicted_count', 0)}")
        print(f"   Overlap:                      {coverage.get('overlap_count', 0)}")
        
        # 5. Calibration
        cal = results.get('calibration', {})
        print("\nüìâ 5. PROBABILITY CALIBRATION")
        print("-" * 80)
        print(f"   Mean Brier Score:            {cal.get('mean_brier_score', 0):.4f}")
        print(f"   Calibration Error:           {cal.get('calibration_error', 0):.4f}")
        
        # 6. Baseline Comparison
        baseline = results.get('baseline', {})
        print("\n‚öñÔ∏è  6. BASELINE COMPARISON")
        print("-" * 80)
        print(f"   Model Hits:                  {baseline.get('model_mean_hits', 0):.3f}")
        print(f"   Random Baseline:             {baseline.get('random_mean_hits', 0):.3f}")
        print(f"   Frequency Baseline:          {baseline.get('frequency_mean_hits', 0):.3f}")
        print(f"   C·∫£i thi·ªán vs Random:         {baseline.get('improvement_over_random', 0):.3f} ({baseline.get('improvement_over_random_pct', 0):.2f}%)")
        print(f"   C·∫£i thi·ªán vs Frequency:       {baseline.get('improvement_over_frequency', 0):.3f}")
        
        print("="*80)
    
    def _get_distribution(self, array: np.ndarray) -> Dict:
        """T√≠nh ph√¢n ph·ªëi c·ªßa array"""
        unique, counts = np.unique(array, return_counts=True)
        total = len(array)
        return {int(k): int(v) for k, v in zip(unique, counts)}

