# src/tuner.py
import optuna
import json
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.multioutput import MultiOutputClassifier
from sklearn.metrics import log_loss
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from catboost import CatBoostClassifier

class UltraTuner:
    def __init__(self, X, y, output_path="data/best_params.json"):
        self.output_path = output_path
        
        # 1. Cáº¯t dá»¯ liá»‡u cÅ© quÃ¡ (giá»¯ 2000 ká»³ cuá»‘i) Ä‘á»ƒ model khÃ´ng há»c nhiá»…u
        limit = 2000
        if len(X) > limit:
            print(f"âœ‚ï¸ [Tuner] Giá»›i háº¡n dá»¯ liá»‡u tuning: {limit} dÃ²ng gáº§n nháº¥t.")
            X_tune = X[-limit:]
            y_tune = y[-limit:]
        else:
            X_tune = X
            y_tune = y

        # 2. CHIA Táº¬P TRAIN/VAL
        self.X_train, self.X_val, self.y_train_full, self.y_val_full = train_test_split(
            X_tune, y_tune, test_size=0.2, random_state=42, shuffle=False
        )

        # 3. CHIáº¾N THUáº¬T Äáº I DIá»†N (PROXY TUNING)
        # Chá»‰ láº¥y 5 cá»™t má»¥c tiÃªu ngáº«u nhiÃªn (vÃ­ dá»¥: index 0, 10, 20, 30, 40) Ä‘á»ƒ Tuning
        # GiÃºp tá»‘c Ä‘á»™ nhanh gáº¥p 9 láº§n (5 models vs 45 models)
        self.proxy_indices = [0, 10, 20, 30, 40] 
        
        # Chá»‰ láº¥y cÃ¡c cá»™t Ä‘áº¡i diá»‡n
        self.y_train = self.y_train_full[:, self.proxy_indices]
        self.y_val = self.y_val_full[:, self.proxy_indices]
        
        print(f"âš¡ [Tuner Strategy] Proxy Mode: Chá»‰ tuning trÃªn {len(self.proxy_indices)}/45 sá»‘ Ä‘áº¡i diá»‡n.")
        
        self.best_params = {}

    def _evaluate_proxy(self, model):
        """
        HÃ m cháº¥m Ä‘iá»ƒm dá»±a trÃªn Log Loss cá»§a 5 cá»™t Ä‘áº¡i diá»‡n.
        Log Loss cÃ ng tháº¥p -> Model cÃ ng tá»± tin vÃ  chÃ­nh xÃ¡c.
        """
        # Train trÃªn 5 cá»™t
        model.fit(self.X_train, self.y_train)
        
        # Predict proba tráº£ vá» list (má»—i pháº§n tá»­ lÃ  proba cho 1 cá»™t)
        probas_list = model.predict_proba(self.X_val)
        
        # Chuyá»ƒn list of arrays -> máº£ng 2D (n_samples, 5)
        # Láº¥y cá»™t index 1 (xÃ¡c suáº¥t ra sá»‘ Ä‘Ã³)
        try:
            val_preds = np.array([p[:, 1] for p in probas_list]).T
        except IndexError:
            # Fallback cho trÆ°á»ng há»£p model dá»± Ä‘oÃ¡n cá»©ng (khÃ´ng ra xÃ¡c suáº¥t)
             val_preds = np.array([p[:, 1] if p.shape[1] > 1 else p[:, 0] for p in probas_list]).T

        # TÃ­nh Log Loss trung bÃ¬nh
        score = log_loss(self.y_val, val_preds)
        return score

    def tune_xgb(self, trial):
        params = {
            'n_estimators': trial.suggest_int('n_estimators', 100, 400),
            'max_depth': trial.suggest_int('max_depth', 3, 8),
            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.2),
            'subsample': trial.suggest_float('subsample', 0.6, 1.0),
            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),
            'tree_method': 'hist', # TÄƒng tá»‘c
            'verbosity': 0,
            'random_state': 42
        }
        model = MultiOutputClassifier(XGBClassifier(**params))
        return self._evaluate_proxy(model)

    def tune_lgb(self, trial):
        params = {
            'n_estimators': trial.suggest_int('n_estimators', 100, 400),
            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.2),
            'num_leaves': trial.suggest_int('num_leaves', 20, 60),
            'feature_fraction': trial.suggest_float('feature_fraction', 0.6, 1.0),
            'bagging_fraction': trial.suggest_float('bagging_fraction', 0.6, 1.0),
            'bagging_freq': trial.suggest_int('bagging_freq', 1, 5),
            'verbose': -1,
            'random_state': 42
        }
        model = MultiOutputClassifier(LGBMClassifier(**params))
        return self._evaluate_proxy(model)

    def tune_cat(self, trial):
        params = {
            'iterations': trial.suggest_int('iterations', 100, 400),
            # CatBoost ráº¥t náº·ng vá»›i depth lá»›n, giá»¯ má»©c 4-7 lÃ  tá»‘i Æ°u cho xá»• sá»‘
            'depth': trial.suggest_int('depth', 4, 7), 
            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.2),
            'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1, 10),
            'border_count': 128, # Giáº£m Ä‘á»™ chi tiáº¿t biÃªn Ä‘á»ƒ tÄƒng tá»‘c
            'thread_count': -1,  # DÃ¹ng full CPU
            'verbose': 0,
            'random_state': 42,
            'allow_writing_files': False
        }
        model = MultiOutputClassifier(CatBoostClassifier(**params))
        return self._evaluate_proxy(model)

    def run_optimization(self, n_trials=20):
        print(f"ğŸ”¥ [Tuner] Báº¯t Ä‘áº§u tá»‘i Æ°u hÃ³a vá»›i {n_trials} trials (Proxy Mode)...")
        
        # 1. XGBoost
        print("   -> ğŸš€ Tuning XGBoost...")
        study_xgb = optuna.create_study(direction='minimize') # LogLoss -> Minimize
        study_xgb.optimize(self.tune_xgb, n_trials=n_trials)
        self.best_params['xgb'] = study_xgb.best_params
        print(f"      âœ… Best LogLoss: {study_xgb.best_value:.4f}")

        # 2. LightGBM
        print("   -> ğŸš€ Tuning LightGBM...")
        study_lgb = optuna.create_study(direction='minimize')
        study_lgb.optimize(self.tune_lgb, n_trials=n_trials)
        self.best_params['lgb'] = study_lgb.best_params
        print(f"      âœ… Best LogLoss: {study_lgb.best_value:.4f}")
        
        # 3. CatBoost
        print("   -> ğŸš€ Tuning CatBoost...")
        study_cat = optuna.create_study(direction='minimize')
        study_cat.optimize(self.tune_cat, n_trials=n_trials) # Giáº£m sá»‘ trial cá»§a Cat náº¿u cáº§n
        self.best_params['cat'] = study_cat.best_params
        print(f"      âœ… Best LogLoss: {study_cat.best_value:.4f}")

        self.save_best_params()

    def save_best_params(self):
        with open(self.output_path, "w") as f:
            json.dump(self.best_params, f, indent=4)
        print(f"ğŸ’¾ [Tuner] ÄÃ£ lÆ°u params vÃ o '{self.output_path}'")