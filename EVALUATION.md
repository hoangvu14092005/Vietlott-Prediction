# üìä T√†i li·ªáu Ph∆∞∆°ng ph√°p ƒê√°nh gi√° Model

## üéØ T·ªïng quan

Module `src/evaluator.py` cung c·∫•p h·ªá th·ªëng ƒë√°nh gi√° to√†n di·ªán v·ªõi nhi·ªÅu metrics ph√π h·ª£p cho b√†i to√°n d·ª± ƒëo√°n x·ªï s·ªë Vietlott 6/45.

## üìà C√°c Metrics ƒë∆∞·ª£c s·ª≠ d·ª•ng

### 1. **Hits Metrics** (S·ªë tr√∫ng)

#### M√¥ t·∫£:
ƒê·∫øm s·ªë l∆∞·ª£ng s·ªë th·ª±c t·∫ø n·∫±m trong Top-K d·ª± ƒëo√°n.

#### Metrics:
- **Mean Hits**: Trung b√¨nh s·ªë tr√∫ng/k·ª≥
- **Std Hits**: ƒê·ªô l·ªách chu·∫©n
- **Median Hits**: Median s·ªë tr√∫ng
- **Min/Max Hits**: S·ªë tr√∫ng th·∫•p nh·∫•t/cao nh·∫•t
- **Hit Rate**: T·ª∑ l·ªá tr√∫ng = Mean Hits / K
- **Hits Distribution**: Ph√¢n ph·ªëi s·ªë tr√∫ng (v√≠ d·ª•: 0 tr√∫ng: 10 k·ª≥, 1 tr√∫ng: 15 k·ª≥...)

#### √ù nghƒ©a:
- **K·ª≥ v·ªçng**: V·ªõi K=8, trung b√¨nh 2-3 s·ªë tr√∫ng/k·ª≥ (~25-37%)
- **T·ªët**: Mean Hits > 2.5 v·ªõi K=8
- **Xu·∫•t s·∫Øc**: Mean Hits > 3.0 v·ªõi K=8

#### V√≠ d·ª•:
```
K·ª≥ 1: D·ª± ƒëo√°n [5, 12, 18, 23, 28, 35, 40, 42]
      K·∫øt qu·∫£ [5, 12, 19, 23, 29, 35]
      ‚Üí Tr√∫ng: 4 s·ªë (5, 12, 23, 35)
```

---

### 2. **Precision, Recall, F1-Score**

#### M√¥ t·∫£:
ƒê√°nh gi√° ƒë·ªô ch√≠nh x√°c c·ªßa d·ª± ƒëo√°n d·ª±a tr√™n classification metrics.

#### Metrics:
- **Precision**: T·ª∑ l·ªá s·ªë d·ª± ƒëo√°n ƒë√∫ng / t·ªïng s·ªë d·ª± ƒëo√°n
  - Precision = TP / (TP + FP)
  - Cao = √≠t false positive
  
- **Recall**: T·ª∑ l·ªá s·ªë th·ª±c t·∫ø ƒë∆∞·ª£c d·ª± ƒëo√°n ƒë√∫ng
  - Recall = TP / (TP + FN)
  - Cao = √≠t false negative
  
- **F1-Score**: Trung b√¨nh ƒëi·ªÅu h√≤a c·ªßa Precision v√† Recall
  - F1 = 2 * (Precision * Recall) / (Precision + Recall)

#### √ù nghƒ©a:
- **Precision cao**: D·ª± ƒëo√°n √≠t sai, nh∆∞ng c√≥ th·ªÉ b·ªè s√≥t s·ªë th·ª±c t·∫ø
- **Recall cao**: B·∫Øt ƒë∆∞·ª£c nhi·ªÅu s·ªë th·ª±c t·∫ø, nh∆∞ng c√≥ th·ªÉ d·ª± ƒëo√°n sai
- **F1 cao**: C√¢n b·∫±ng gi·ªØa Precision v√† Recall

#### K·ª≥ v·ªçng:
- Precision: ~0.3-0.4 (v√¨ ch·ªâ d·ª± ƒëo√°n 8/45 s·ªë)
- Recall: ~0.5-0.7 (b·∫Øt ƒë∆∞·ª£c 3-4/6 s·ªë th·ª±c t·∫ø)
- F1: ~0.4-0.5

---

### 3. **Rank Metrics** (V·ªã tr√≠ trong Ranking)

#### M√¥ t·∫£:
ƒê√°nh gi√° v·ªã tr√≠ c·ªßa s·ªë th·ª±c t·∫ø trong ranking x√°c su·∫•t.

#### Metrics:
- **Mean Rank**: Trung b√¨nh v·ªã tr√≠ c·ªßa s·ªë th·ª±c t·∫ø trong ranking
- **Median Rank**: Median v·ªã tr√≠
- **Mean Rank in Top-K**: Trung b√¨nh v·ªã tr√≠ c·ªßa s·ªë th·ª±c t·∫ø n·∫±m trong Top-K
- **Coverage at K**: T·ª∑ l·ªá s·ªë th·ª±c t·∫ø n·∫±m trong Top-K
- **Min/Max Rank**: V·ªã tr√≠ th·∫•p nh·∫•t/cao nh·∫•t

#### √ù nghƒ©a:
- **Rank th·∫•p** (1-8): S·ªë th·ª±c t·∫ø c√≥ x√°c su·∫•t cao ‚Üí Model t·ªët
- **Rank cao** (>20): S·ªë th·ª±c t·∫ø c√≥ x√°c su·∫•t th·∫•p ‚Üí Model ch∆∞a t·ªët
- **Coverage cao**: Nhi·ªÅu s·ªë th·ª±c t·∫ø n·∫±m trong Top-K

#### K·ª≥ v·ªçng:
- Mean Rank: 15-20 (gi·ªØa 45 s·ªë)
- Coverage at K: 50-70% (3-4/6 s·ªë th·ª±c t·∫ø trong Top-K)

---

### 4. **Coverage Metrics** (ƒê·ªô bao ph·ªß)

#### M√¥ t·∫£:
ƒê√°nh gi√° bao nhi√™u s·ªë th·ª±c t·∫ø ƒë∆∞·ª£c d·ª± ƒëo√°n.

#### Metrics:
- **Mean Coverage**: Trung b√¨nh coverage/k·ª≥
  - Coverage = S·ªë tr√∫ng / T·ªïng s·ªë th·ª±c t·∫ø
  
- **Total Coverage**: Coverage t·ªïng th·ªÉ
  - T·ª∑ l·ªá s·ªë th·ª±c t·∫ø unique ƒë∆∞·ª£c d·ª± ƒëo√°n trong to√†n b·ªô test set
  
- **Unique Counts**: S·ªë l∆∞·ª£ng s·ªë th·ª±c t·∫ø/d·ª± ƒëo√°n unique

#### √ù nghƒ©a:
- **Coverage cao**: Model b·∫Øt ƒë∆∞·ª£c nhi·ªÅu s·ªë th·ª±c t·∫ø
- **Total Coverage**: Cho bi·∫øt model c√≥ bias v·ªÅ m·ªôt s·ªë s·ªë c·ª• th·ªÉ kh√¥ng

#### K·ª≥ v·ªçng:
- Mean Coverage: 50-70% (3-4/6 s·ªë th·ª±c t·∫ø)
- Total Coverage: 60-80% (nhi·ªÅu s·ªë th·ª±c t·∫ø ƒë∆∞·ª£c d·ª± ƒëo√°n √≠t nh·∫•t 1 l·∫ßn)

---

### 5. **Probability Calibration** (Hi·ªáu chu·∫©n X√°c su·∫•t)

#### M√¥ t·∫£:
ƒê√°nh gi√° ƒë·ªô ch√≠nh x√°c c·ªßa x√°c su·∫•t d·ª± ƒëo√°n.

#### Metrics:
- **Brier Score**: ƒê·ªô l·ªói b√¨nh ph∆∞∆°ng trung b√¨nh
  - Brier = mean((predicted_prob - actual)^2)
  - Th·∫•p h∆°n = t·ªët h∆°n (0 = ho√†n h·∫£o)
  
- **Calibration Error**: ƒê·ªô l·ªách gi·ªØa x√°c su·∫•t d·ª± ƒëo√°n v√† x√°c su·∫•t th·ª±c t·∫ø
  - So s√°nh predicted probability vs empirical probability trong c√°c bins

#### √ù nghƒ©a:
- **Brier Score th·∫•p**: X√°c su·∫•t d·ª± ƒëo√°n g·∫ßn v·ªõi th·ª±c t·∫ø
- **Calibration Error th·∫•p**: X√°c su·∫•t ƒë∆∞·ª£c hi·ªáu chu·∫©n t·ªët

#### K·ª≥ v·ªçng:
- Brier Score: 0.08-0.12 (cho b√†i to√°n multi-label)
- Calibration Error: < 0.05

---

### 6. **Baseline Comparison** (So s√°nh v·ªõi Baseline)

#### M√¥ t·∫£:
So s√°nh model v·ªõi c√°c ph∆∞∆°ng ph√°p baseline ƒë∆°n gi·∫£n.

#### Baselines:
1. **Random Baseline**: Ch·ªçn ng·∫´u nhi√™n K s·ªë
2. **Frequency Baseline**: Ch·ªçn K s·ªë c√≥ t·∫ßn su·∫•t cao nh·∫•t trong training data

#### Metrics:
- **Model Hits**: S·ªë tr√∫ng c·ªßa model
- **Random Hits**: S·ªë tr√∫ng c·ªßa random baseline
- **Frequency Hits**: S·ªë tr√∫ng c·ªßa frequency baseline
- **Improvement**: C·∫£i thi·ªán so v·ªõi baseline

#### √ù nghƒ©a:
- **Improvement > 0**: Model t·ªët h∆°n baseline
- **Improvement > 50%**: Model t·ªët h∆°n ƒë√°ng k·ªÉ

#### K·ª≥ v·ªçng:
- Random Baseline: ~1.07 hits/k·ª≥ (K=8, 6 s·ªë th·ª±c t·∫ø)
- Frequency Baseline: ~1.5-2.0 hits/k·ª≥
- Model: > 2.0 hits/k·ª≥ (c·∫£i thi·ªán 30-50%)

---

## üîÑ Quy tr√¨nh ƒê√°nh gi√°

### 1. **Train/Test Split**
```
Total Data: N k·ª≥
‚îú‚îÄ‚îÄ Train: N - TEST_SIZE k·ª≥ (ƒë·ªÉ train model)
‚îî‚îÄ‚îÄ Test: TEST_SIZE k·ª≥ cu·ªëi (ƒë·ªÉ ƒë√°nh gi√°)
```

### 2. **Time-Series Split**
- **Quan tr·ªçng**: Kh√¥ng shuffle d·ªØ li·ªáu
- Test set ph·∫£i l√† c√°c k·ª≥ **sau** training set
- Tr√°nh data leakage

### 3. **Evaluation Process**
```python
# 1. D·ª± b√°o cho test set
predictions = model.predict(test_features)  # (n_samples, 45)

# 2. ƒê√°nh gi√° to√†n di·ªán
evaluator = LotteryEvaluator(top_k=8)
results = evaluator.comprehensive_evaluate(predictions, test_labels)

# 3. In b√°o c√°o
evaluator.print_evaluation_report(results)
```

---

## üìä C√°ch ƒê·ªçc B√°o c√°o

### V√≠ d·ª• Output:
```
üìä B√ÅO C√ÅO ƒê√ÅNH GI√Å TO√ÄN DI·ªÜN
================================================================================

üéØ 1. HITS METRICS (S·ªë tr√∫ng)
--------------------------------------------------------------------------------
   Trung b√¨nh s·ªë tr√∫ng/k·ª≥:     2.450 ¬± 0.850
   Median s·ªë tr√∫ng:            2.00
   Min/Max:                     0 / 5
   Hit Rate (tr√∫ng/K):          30.62%
   Ph√¢n ph·ªëi:                  {0: 5, 1: 12, 2: 15, 3: 10, 4: 6, 5: 2}

üìà 2. PRECISION/RECALL/F1
--------------------------------------------------------------------------------
   Precision:                   0.3062 ¬± 0.0850
   Recall:                     0.4083 ¬± 0.1133
   F1-Score:                   0.3500 ¬± 0.0950

üìä 3. RANK METRICS
--------------------------------------------------------------------------------
   Mean Rank:                   18.50
   Median Rank:                 16.00
   Mean Rank trong Top-8:       4.25
   Coverage t·∫°i Top-8:          65.00%

üéØ 4. COVERAGE
--------------------------------------------------------------------------------
   Mean Coverage:               40.83%
   Total Coverage:              73.33%
   S·ªë th·ª±c t·∫ø unique:          45
   S·ªë d·ª± ƒëo√°n unique:          42
   Overlap:                     33

üìâ 5. PROBABILITY CALIBRATION
--------------------------------------------------------------------------------
   Mean Brier Score:            0.0950
   Calibration Error:           0.0320

‚öñÔ∏è  6. BASELINE COMPARISON
--------------------------------------------------------------------------------
   Model Hits:                  2.450
   Random Baseline:             1.067
   Frequency Baseline:          1.850
   C·∫£i thi·ªán vs Random:         1.383 (129.62%)
   C·∫£i thi·ªán vs Frequency:      0.600
```

### Gi·∫£i th√≠ch:
- **Hits**: Model trung b√¨nh tr√∫ng 2.45 s·ªë/k·ª≥ (t·ªët!)
- **Precision/Recall**: C√¢n b·∫±ng, kh√¥ng qu√° thi√™n v·ªÅ m·ªôt ph√≠a
- **Rank**: S·ªë th·ª±c t·∫ø th∆∞·ªùng n·∫±m ·ªü v·ªã tr√≠ 18.5 (trung b√¨nh), nh∆∞ng 65% n·∫±m trong Top-8
- **Coverage**: B·∫Øt ƒë∆∞·ª£c 40.83% s·ªë th·ª±c t·∫ø m·ªói k·ª≥, t·ªïng th·ªÉ 73.33%
- **Calibration**: X√°c su·∫•t kh√° ch√≠nh x√°c (Brier = 0.095)
- **Baseline**: T·ªët h∆°n Random 129%, t·ªët h∆°n Frequency 32%

---

## üéØ Ti√™u ch√≠ ƒê√°nh gi√° T·ªïng th·ªÉ

### Model T·ªët:
- ‚úÖ Mean Hits > 2.5 (v·ªõi K=8)
- ‚úÖ Hit Rate > 30%
- ‚úÖ Coverage > 60%
- ‚úÖ Improvement vs Random > 100%
- ‚úÖ Brier Score < 0.10

### Model Xu·∫•t s·∫Øc:
- ‚úÖ Mean Hits > 3.0
- ‚úÖ Hit Rate > 37%
- ‚úÖ Coverage > 70%
- ‚úÖ Improvement vs Random > 150%
- ‚úÖ Brier Score < 0.08

### Model C·∫ßn C·∫£i thi·ªán:
- ‚ö†Ô∏è Mean Hits < 2.0
- ‚ö†Ô∏è Hit Rate < 25%
- ‚ö†Ô∏è Coverage < 50%
- ‚ö†Ô∏è Improvement vs Random < 50%

---

## üîß T√πy ch·ªânh ƒê√°nh gi√°

### Thay ƒë·ªïi Top-K:
```python
evaluator = LotteryEvaluator(top_k=10)  # Thay v√¨ 8
```

### Ch·ªâ ƒë√°nh gi√° m·ªôt s·ªë metrics:
```python
# Ch·ªâ ƒë√°nh gi√° hits
hits_results = evaluator.evaluate_hits(predictions, actuals)

# Ch·ªâ ƒë√°nh gi√° rank
rank_results = evaluator.evaluate_rank_metrics(predictions, actuals)
```

### So s√°nh nhi·ªÅu models:
```python
results_model1 = evaluator.comprehensive_evaluate(pred1, actuals)
results_model2 = evaluator.comprehensive_evaluate(pred2, actuals)

# So s√°nh
print(f"Model 1 Hits: {results_model1['hits']['mean_hits']:.3f}")
print(f"Model 2 Hits: {results_model2['hits']['mean_hits']:.3f}")
```

---

## ‚ö†Ô∏è L∆∞u √Ω Quan tr·ªçng

1. **X·ªï s·ªë l√† ng·∫´u nhi√™n**: Kh√¥ng th·ªÉ ƒë·∫°t 100% accuracy
2. **Overfitting**: C·∫ßn ki·ªÉm tra performance tr√™n validation set
3. **Time-series**: Kh√¥ng shuffle d·ªØ li·ªáu, test ph·∫£i sau train
4. **Baseline**: Lu√¥n so s√°nh v·ªõi baseline ƒë·ªÉ bi·∫øt model c√≥ th·ª±c s·ª± t·ªët kh√¥ng
5. **Multiple Metrics**: D√πng nhi·ªÅu metrics ƒë·ªÉ ƒë√°nh gi√° to√†n di·ªán

---

## üìö T√†i li·ªáu Tham kh·∫£o

- **Brier Score**: https://en.wikipedia.org/wiki/Brier_score
- **Precision/Recall**: https://en.wikipedia.org/wiki/Precision_and_recall
- **Calibration**: https://scikit-learn.org/stable/modules/calibration.html

