# ğŸ¯ Giáº£i phÃ¡p Logic cho BÃ i toÃ¡n Dá»± Ä‘oÃ¡n Vietlott 6/45

## ğŸ“‹ Tá»•ng quan

Há»‡ thá»‘ng dá»± Ä‘oÃ¡n sá»‘ Vietlott 6/45 sá»­ dá»¥ng Machine Learning vá»›i phÆ°Æ¡ng phÃ¡p Ensemble Ä‘á»ƒ káº¿t há»£p nhiá»u models vÃ  Ä‘Æ°a ra dá»± Ä‘oÃ¡n tá»‘t nháº¥t.

## ğŸ§  Logic Giáº£i quyáº¿t BÃ i toÃ¡n

### 1. **Feature Engineering (Táº¡o Äáº·c trÆ°ng)**

#### a) Táº§n suáº¥t & Gáº§n Ä‘Ã¢y (Gan)
- **Táº§n suáº¥t**: Äáº¿m sá»‘ láº§n má»—i sá»‘ (1-45) xuáº¥t hiá»‡n trong N ká»³ quay gáº§n nháº¥t
- **Gan (Last Seen)**: Sá»‘ ká»³ gáº§n nháº¥t má»—i sá»‘ xuáº¥t hiá»‡n
- **Logic**: Sá»‘ cÃ³ táº§n suáº¥t cao hoáº·c "gan" (lÃ¢u khÃ´ng ra) cÃ³ thá»ƒ cÃ³ xÃ¡c suáº¥t cao hÆ¡n

#### b) FFT Signals (PhÃ¢n tÃ­ch Chu ká»³)
- Sá»­ dá»¥ng **Biáº¿n Ä‘á»•i Fourier** Ä‘á»ƒ phÃ¡t hiá»‡n chu ká»³ áº©n trong chuá»—i xuáº¥t hiá»‡n cá»§a má»—i sá»‘
- **Logic**: Náº¿u má»™t sá»‘ cÃ³ chu ká»³ xuáº¥t hiá»‡n Ä‘á»u Ä‘áº·n, cÃ³ thá»ƒ dá»± Ä‘oÃ¡n Ä‘Æ°á»£c thá»i Ä‘iá»ƒm tiáº¿p theo

#### c) Delta & Skewness (Cáº¥u trÃºc Bá»™ sá»‘)
- **Delta**: Khoáº£ng cÃ¡ch giá»¯a cÃ¡c sá»‘ trong ká»³ quay gáº§n nháº¥t (vÃ­ dá»¥: 5, 12, 18 â†’ Delta: 7, 6)
- **Skewness**: Äá»™ lá»‡ch phÃ¢n phá»‘i sá»‘ (sá»‘ táº­p trung á»Ÿ Ä‘áº§u/cuá»‘i hay ráº£i Ä‘á»u)
- **Logic**: PhÃ¢n tÃ­ch pattern cáº¥u trÃºc cá»§a bá»™ sá»‘ Ä‘á»ƒ hiá»ƒu xu hÆ°á»›ng

#### d) Poisson Probability (XÃ¡c suáº¥t LÃ½ thuyáº¿t)
- So sÃ¡nh xÃ¡c suáº¥t thá»±c táº¿ vs lÃ½ thuyáº¿t (6/45 = 0.133)
- **Logic**: Sá»‘ cÃ³ xÃ¡c suáº¥t thá»±c táº¿ lá»‡ch nhiá»u so vá»›i lÃ½ thuyáº¿t cÃ³ thá»ƒ "bÃ¹ trá»«" trong tÆ°Æ¡ng lai

### 2. **Multi-Model Ensemble**

#### Táº¡i sao dÃ¹ng Ensemble?
- **Giáº£m Bias**: Má»—i model cÃ³ bias khÃ¡c nhau, káº¿t há»£p giáº£m bias tá»•ng thá»ƒ
- **Giáº£m Variance**: Káº¿t quáº£ á»•n Ä‘á»‹nh hÆ¡n khi káº¿t há»£p nhiá»u models
- **Táº­n dá»¥ng Ä‘iá»ƒm máº¡nh**: Má»—i model cÃ³ tháº¿ máº¡nh riÃªng

#### CÃ¡c Models Ä‘Æ°á»£c sá»­ dá»¥ng:
1. **XGBoost**: Gradient Boosting máº¡nh, xá»­ lÃ½ non-linear tá»‘t
2. **LightGBM**: Nhanh, hiá»‡u quáº£ vá»›i dá»¯ liá»‡u lá»›n
3. **CatBoost**: Xá»­ lÃ½ categorical features tá»‘t, Ã­t overfitting
4. **TabNet**: Deep Learning cho tabular data, há»c Ä‘Æ°á»£c pattern phá»©c táº¡p
5. **Random Forest**: Ensemble tree-based, robust
6. **Logistic Regression**: Baseline, Ä‘Æ¡n giáº£n nhÆ°ng hiá»‡u quáº£

### 3. **Probability Averaging**

- Má»—i model dá»± Ä‘oÃ¡n xÃ¡c suáº¥t cho 45 sá»‘ (1-45)
- **Káº¿t há»£p**: Láº¥y trung bÃ¬nh xÃ¡c suáº¥t tá»« táº¥t cáº£ models
- **Logic**: XÃ¡c suáº¥t trung bÃ¬nh tá»« nhiá»u models Ä‘Ã¡ng tin cáº­y hÆ¡n xÃ¡c suáº¥t tá»« 1 model

### 4. **Top-K Selection**

- Chá»n K sá»‘ (máº·c Ä‘á»‹nh K=8) cÃ³ xÃ¡c suáº¥t cao nháº¥t
- **Logic**: KhÃ´ng dá»± Ä‘oÃ¡n chÃ­nh xÃ¡c 6 sá»‘, mÃ  Ä‘á» xuáº¥t K sá»‘ cÃ³ kháº£ nÄƒng cao nháº¥t

## ğŸ”„ Quy trÃ¬nh Hoáº¡t Ä‘á»™ng

```
1. Load Data (lottery_data.npy)
   â†“
2. Feature Engineering (Táº¡o 300+ features tá»« lá»‹ch sá»­)
   â†“
3. Train/Load Models (6 models ensemble)
   â†“
4. Predict (TÃ­nh xÃ¡c suáº¥t cho 45 sá»‘)
   â†“
5. Select Top-K (Chá»n 8 sá»‘ cÃ³ xÃ¡c suáº¥t cao nháº¥t)
   â†“
6. Output (Äá» xuáº¥t bá»™ sá»‘)
```

## ğŸ“Š ÄÃ¡nh giÃ¡ Hiá»‡u suáº¥t

- **Metric**: Sá»‘ lÆ°á»£ng sá»‘ trÃºng trong Top-K
- **Test**: DÃ¹ng N ká»³ cuá»‘i (máº·c Ä‘á»‹nh 50) Ä‘á»ƒ kiá»ƒm tra
- **Ká»³ vá»ng**: Trung bÃ¬nh 2-3 sá»‘ trÃºng trong Top-8 (tá»· lá»‡ ~25-37%)

## âš ï¸ LÆ°u Ã½ Quan trá»ng

1. **Xá»• sá»‘ lÃ  ngáº«u nhiÃªn**: Model khÃ´ng thá»ƒ dá»± Ä‘oÃ¡n chÃ­nh xÃ¡c 100%, chá»‰ phÃ¢n tÃ­ch pattern vÃ  Ä‘Æ°a ra gá»£i Ã½
2. **Overfitting**: Cáº§n cáº©n tháº­n vá»›i overfitting - model cÃ³ thá»ƒ "nhá»›" dá»¯ liá»‡u train nhÆ°ng khÃ´ng generalize tá»‘t
3. **Data Quality**: Cháº¥t lÆ°á»£ng dá»¯ liá»‡u áº£nh hÆ°á»Ÿng lá»›n Ä‘áº¿n káº¿t quáº£
4. **Hyperparameter Tuning**: Tá»‘i Æ°u tham sá»‘ giÃºp cáº£i thiá»‡n hiá»‡u suáº¥t Ä‘Ã¡ng ká»ƒ

## ğŸš€ Cáº£i tiáº¿n Tiá»m nÄƒng

1. **Time Series Features**: ThÃªm features vá» thá»i gian (ngÃ y trong tuáº§n, thÃ¡ng, etc.)
2. **Pair/Triplet Analysis**: PhÃ¢n tÃ­ch cáº·p sá»‘, bá»™ 3 sá»‘ thÆ°á»ng xuáº¥t hiá»‡n cÃ¹ng nhau
3. **Hot/Cold Numbers**: PhÃ¢n loáº¡i sá»‘ "nÃ³ng" (xuáº¥t hiá»‡n nhiá»u) vÃ  "láº¡nh" (Ã­t xuáº¥t hiá»‡n)
4. **Sequence Patterns**: PhÃ¢n tÃ­ch chuá»—i sá»‘ liÃªn tiáº¿p, sá»‘ cháºµn/láº»
5. **Cross-Validation**: Sá»­ dá»¥ng time-series cross-validation thay vÃ¬ random split

## ğŸ“ˆ Káº¿t luáº­n

Há»‡ thá»‘ng sá»­ dá»¥ng:
- **Feature Engineering** Ä‘á»ƒ trÃ­ch xuáº¥t thÃ´ng tin tá»« lá»‹ch sá»­
- **Ensemble Learning** Ä‘á»ƒ káº¿t há»£p sá»©c máº¡nh cá»§a nhiá»u models
- **Probability-based Selection** Ä‘á»ƒ Ä‘Æ°a ra gá»£i Ã½ há»£p lÃ½

Máº·c dÃ¹ khÃ´ng thá»ƒ Ä‘áº£m báº£o trÃºng 100%, nhÆ°ng phÆ°Æ¡ng phÃ¡p nÃ y giÃºp phÃ¢n tÃ­ch dá»¯ liá»‡u má»™t cÃ¡ch cÃ³ há»‡ thá»‘ng vÃ  Ä‘Æ°a ra gá»£i Ã½ dá»±a trÃªn pattern lá»‹ch sá»­.

